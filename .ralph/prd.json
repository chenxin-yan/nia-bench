{
  "tasks": [
    {
      "description": "Initialize project scaffolding with Bun, TypeScript strict mode, and directory structure",
      "subtasks": [
        "Run `bun init` to create the project with TypeScript support",
        "Configure `tsconfig.json` with strict mode, ES2022 target, moduleResolution: bundler, path aliases (`@/` -> `src/`)",
        "Create the directory structure as defined in BENCHMARK.md Section 8: `src/`, `tasks/bleeding_edge/`, `tasks/version_locked_write/`, `tasks/version_locked_audit/`, `src/runner/`, `src/runner/mcp_configs/`, `src/tests/`, `src/judge/`, `results/`, `reference/`, `typecheck-envs/`",
        "Install core dependencies: `ts-morph`, `zod` (for schema validation), and `openai` (for OpenRouter API calls via OpenAI-compatible client)",
        "Add scripts to `package.json`: `bench` (main runner), `typecheck` (`tsc --noEmit`), `test` (`bun test`), `lint` (biome), `format` (biome)",
        "Install and configure Biome for linting and formatting with a `biome.json` config",
        "Create `src/index.ts` as the CLI entry point with a placeholder that logs 'nia-bench' and exits",
        "Create a `.env.example` with required env vars: `OPENROUTER_API_KEY`, `OPENAI_API_KEY`, `ANTHROPIC_API_KEY`",
        "Run `bun run typecheck` to verify TypeScript configuration",
        "Run `bun run lint` to ensure code quality",
        "Run `bun run bench` and verify it prints 'nia-bench' and exits with code 0 — confirms the CLI entry point and package.json script are wired correctly",
        "Run `bun test` to verify the test runner is configured (should report 0 tests, no errors)"
      ],
      "notes": "Use Bun runtime throughout. The CLI entry point will be expanded in later tasks. Path aliases help keep imports clean. Biome replaces ESLint+Prettier as a single fast tool. See SPEC.md Technical Stack section.",
      "passed": true
    },
    {
      "description": "Define task JSON schema with Zod and create the task loader module",
      "subtasks": [
        "Create `src/types/task.ts` with Zod schemas for the task JSON format: `TaskSchema` with fields `id`, `category` (enum: bleeding_edge, version_locked_write, version_locked_audit), `library` (enum: next, react, ai, trpc, zod), `target_version`, `prompt`, `context` (optional object with `code` and `package_json` fields), `reference_solution`, `test_spec` (object with `ast_checks` array and `type_check` boolean), `rubric` (object with `criteria` array of `{name, weight, description}`), `common_hallucinations` (string array)",
        "Export inferred TypeScript types from the Zod schemas: `Task`, `AstCheck`, `RubricCriterion`, etc.",
        "Create `src/loader/task-loader.ts` that reads all JSON files from `tasks/` subdirectories, validates each against the Zod schema, and returns typed `Task[]`",
        "Add filtering support to the loader: filter by `category`, `library`, or `id` (single task)",
        "Add error reporting: if a task JSON fails validation, log the file path and Zod error details, then skip it (don't crash)",
        "Create `src/loader/index.ts` that re-exports the loader",
        "Write a simple test in `src/loader/__tests__/task-loader.test.ts` that creates a temp directory with a valid and an invalid JSON file, runs the loader, and verifies it loads the valid one and reports the invalid one",
        "Run `bun test` to verify the test passes",
        "Run `bun run typecheck` to ensure no type errors",
        "Run `bun run lint` to ensure code quality"
      ],
      "notes": "The task schema must match BENCHMARK.md Section 3.2. The `test_spec.ast_checks` array contains objects like `{type: 'import_exists' | 'import_absent' | 'function_exported' | 'function_absent' | 'await_present' | 'await_absent' | 'call_exists' | 'call_absent' | 'directive_present' | 'type_check', ...params}`. Each AST check type has different params — define a discriminated union. See BENCHMARK.md task examples for all check types needed.",
      "passed": true
    },
    {
      "description": "Create 5 pilot task JSON files covering all three categories and multiple libraries",
      "subtasks": [
        "Create `tasks/bleeding_edge/nextjs-16-proxy-ts.json` — Task A-NX-1 from BENCHMARK.md: proxy.ts middleware rename. Include full prompt, reference_solution, test_spec with AST checks (file named proxy.ts, exports proxy function, no middleware export, config.matcher present, no edge runtime), rubric criteria, and common_hallucinations",
        "Create `tasks/bleeding_edge/react-19-use-hook.json` — Task A-RX-1: use() hook. Include full prompt, reference_solution, test_spec (imports use from react, calls use() with promise, Suspense wraps component, no useEffect pattern), rubric, hallucinations",
        "Create `tasks/version_locked_write/nextjs-13-sync-request-apis.json` — Task B1-NX-1: sync cookies/headers in v13. Include prompt, reference_solution, test_spec (cookies() not awaited, headers() not awaited, imports from next/headers), rubric, hallucinations",
        "Create `tasks/version_locked_write/react-17-render-entry.json` — Task B1-RX-2: ReactDOM.render entry point. Include prompt, reference_solution, test_spec (calls ReactDOM.render, imports from react-dom not react-dom/client, no createRoot), rubric, hallucinations",
        "Create `tasks/version_locked_audit/react-17-audit-v19-code.json` — Task B2-RX-1: audit React 19 code for v17 compatibility. Include prompt with code snippet, reference_solution listing all issues, rubric criteria for each issue identification",
        "Validate all 5 task files load successfully with the task loader from the previous task",
        "Run `bun run typecheck` to ensure no type errors",
        "Run `bun run lint` to ensure code quality"
      ],
      "notes": "These 5 pilot tasks are chosen to cover: 2 bleeding-edge (Next.js 16, React 19), 2 version-locked-write (Next.js 13, React 17), 1 version-locked-audit (React 17). They span 2 libraries (Next.js, React) and all 3 categories. Full task content is in BENCHMARK.md Section 4. Copy prompts, reference solutions, test specs, and rubrics verbatim from BENCHMARK.md. IMPORTANT: The task JSON must match the Zod schema in `src/types/task.ts`. Key points: (1) AST checks use a discriminated union on `type` field — see the 16 supported types in task.ts, (2) rubric criterion `weight` is a float 0-1 (e.g., 0.25 = 25%), (3) audit tasks should have `test_spec.ast_checks` as an empty array `[]`, (4) use `loadTasks()` from `src/loader` to validate all 5 files load correctly. You can write a quick validation script or test.",
      "passed": true
    },
    {
      "description": "Build the AST checker engine using ts-morph for automated test assertions",
      "subtasks": [
        "Create `src/tests/ast-checker.ts` with a function `runAstChecks(code: string, checks: AstCheck[]): AstCheckResult[]` that parses code with ts-morph and runs each check",
        "Implement AST check type `import_exists`: verify a specific named import from a specific module exists (e.g., `use` from `react`)",
        "Implement AST check type `import_absent`: verify a specific named import does NOT exist (e.g., `useFormState` should not be imported)",
        "Implement AST check type `module_import_absent`: verify no imports from a specific module (e.g., nothing from `react-dom/client`)",
        "Implement AST check type `function_exported`: verify a named function is exported (e.g., `export function proxy`)",
        "Implement AST check type `function_absent`: verify a named function is NOT exported (e.g., no `export function middleware`)",
        "Implement AST check type `await_present`: verify a specific function call IS awaited (e.g., `await params`, `await cookies()`)",
        "Implement AST check type `await_absent`: verify a specific function call is NOT awaited (e.g., `streamText()` without await)",
        "Implement AST check type `call_exists`: verify a function/method call exists (e.g., `use(commentsPromise)`, `cacheLife('hours')`)",
        "Implement AST check type `call_absent`: verify a function/method call does NOT exist (e.g., no `toDataStreamResponse()`)",
        "Implement AST check type `directive_present`: verify a string directive exists at file level (e.g., `'use cache'`, `'use server'`)",
        "Implement AST check type `property_location`: verify a property is inside a specific call expression (e.g., `transformer` inside `httpBatchLink({})`)",
        "Implement AST check type `async_function`: verify a function is async (e.g., the page component)",
        "Implement AST check type `async_generator`: verify a function uses `async function*` pattern",
        "Implement AST check type `yield_present`: verify `yield` keyword is used inside a function",
        "Implement AST check type `type_annotation`: verify a parameter has a specific type annotation (e.g., `params: Promise<{ id: string }>`)",
        "Implement AST check type `property_absent`: verify a specific property does NOT exist in an object literal (e.g., no `runtime: 'edge'` in config)",
        "Create `src/tests/index.ts` re-exporting the checker",
        "Define `AstCheckResult` type: `{check: AstCheck, passed: boolean, message: string}`",
        "Test the AST checker against the 5 pilot task reference solutions — verify all checks PASS on correct reference code",
        "Write negative test cases: for each pilot task, create a known-BAD code variant (e.g., `middleware.ts` instead of `proxy.ts`, `await` where it shouldn't be) and verify the appropriate AST checks correctly FAIL. This validates the checker catches real hallucinations",
        "Write a test file `src/tests/__tests__/ast-checker.test.ts` covering at least: 1 positive case (reference code passes all checks), 1 negative case (hallucinated code fails expected checks), and 1 edge case (empty code string, malformed code)",
        "Run `bun test` to verify all AST checker tests pass",
        "Run `bun run typecheck` to ensure no type errors",
        "Run `bun run lint` to ensure code quality"
      ],
      "notes": "Use ts-morph's Project and SourceFile APIs. Create an in-memory project (no need to write files to disk) — use `project.createSourceFile('temp.ts', code)`. Each check type maps to a specific AST traversal pattern. The discriminated union in the task schema (from task 2) defines what params each check type accepts. Keep the checker stateless — it receives code string + checks array and returns results. IMPORTANT: Testing only positive cases is insufficient — you MUST verify the checker correctly rejects bad code. For example, test that `import_absent('useFormState')` fails when code contains `import { useFormState } from 'react-dom'`. See BENCHMARK.md Section 5.1 for the full list of test types. NOTE from pilot tasks: (1) `call_exists` with `config.matcher` checks for property access in an exported object — needs to handle object property detection, (2) `call_exists` with `Suspense` needs to detect JSX element usage like `<Suspense>`, (3) `call_exists` with `ReactDOM.render` needs to handle method calls on objects (property access calls), (4) `call_exists` with `use` needs to detect plain function calls. The 5 pilot tasks use these check types: function_exported, function_absent, call_exists, property_absent, import_exists, import_absent, await_absent, module_import_absent. Test against all 5 pilot task reference solutions.",
      "passed": true
    },
    {
      "description": "Create version-grouped type-check environments with pinned library versions",
      "subtasks": [
        "Create `typecheck-envs/` directory structure with one subdirectory per library+version combination",
        "Create `typecheck-envs/next-13/package.json` with `next@13.x.x` (latest 13 patch), `react@18.x.x`, `@types/react`, `typescript` pinned",
        "Create `typecheck-envs/next-14/package.json` with `next@14.x.x`, `react@18.x.x`, `@types/react`, `typescript` pinned",
        "Create `typecheck-envs/next-15/package.json` with `next@15.x.x`, `react@18.x.x` or `react@19.x.x`, `@types/react`, `typescript` pinned",
        "Create `typecheck-envs/next-16/package.json` with `next@16.x.x`, `react@19.x.x`, `@types/react`, `typescript` pinned",
        "Create `typecheck-envs/react-17/package.json` with `react@17.x.x`, `react-dom@17.x.x`, `@types/react@17`, `@types/react-dom@17`, `typescript` pinned",
        "Create `typecheck-envs/react-18/package.json` with `react@18.x.x`, `react-dom@18.x.x`, `@types/react@18`, `@types/react-dom@18`, `typescript` pinned",
        "Create `typecheck-envs/react-19/package.json` with `react@19.x.x`, `react-dom@19.x.x`, `@types/react@19`, `@types/react-dom@19`, `typescript` pinned",
        "Create `typecheck-envs/ai-sdk-3/package.json` with `ai@3.x.x`, `@ai-sdk/openai@0.x.x` (matching v3), `typescript` pinned",
        "Create `typecheck-envs/ai-sdk-4/package.json` with `ai@4.x.x`, `@ai-sdk/openai` (matching v4), `typescript` pinned",
        "Create `typecheck-envs/ai-sdk-5/package.json` with `ai@5.x.x`, `@ai-sdk/openai` (matching v5), `typescript` pinned",
        "Create `typecheck-envs/trpc-10/package.json` with `@trpc/server@10.x.x`, `@trpc/client@10.x.x`, `@trpc/react-query@10.x.x`, `superjson`, `zod`, `typescript` pinned",
        "Create `typecheck-envs/trpc-11/package.json` with `@trpc/server@11.x.x`, `@trpc/client@11.x.x`, `superjson`, `zod`, `typescript` pinned",
        "Create `typecheck-envs/zod-3/package.json` with `zod@3.x.x`, `typescript` pinned",
        "Create `typecheck-envs/zod-4/package.json` with `zod@4.x.x`, `typescript` pinned",
        "Create a minimal `tsconfig.json` in each environment directory with strict mode, jsx: react-jsx, noEmit: true",
        "Run `bun install` in each environment directory to generate lockfiles and verify dependency resolution",
        "Create `src/tests/type-checker.ts` with a function `runTypeCheck(code: string, libraryVersion: {library: string, version: string}): TypeCheckResult` that writes the code to the appropriate env directory, runs `tsc --noEmit`, and returns pass/fail with error messages",
        "Test the type checker with a known-good reference solution from the pilot tasks — verify it returns `{passed: true, errors: []}`",
        "Test the type checker with a known-BAD code sample (e.g., use `await cookies()` against Next.js 13 env which expects sync) — verify it returns `{passed: false, errors: [...]}` with meaningful error messages",
        "Write a test file `src/tests/__tests__/type-checker.test.ts` covering: 1 positive case (correct code passes), 1 negative case (version-mismatched code fails), 1 edge case (syntax error in code)",
        "Run `bun test` to verify all type checker tests pass",
        "Run `bun run typecheck` to ensure no type errors in the main project",
        "Run `bun run lint` to ensure code quality"
      ],
      "notes": "Use exact version specifiers (e.g., `next@13.5.6`) not ranges, to ensure reproducibility. The type checker should copy the generated code into a temp file inside the appropriate env directory, run `tsc --noEmit` via Bun's shell, and parse the output. Clean up temp files after checking. For some libraries (AI SDK v3/v4/v5), verify which exact versions exist on npm before pinning. The type checker returns `{passed: boolean, errors: string[]}`. IMPORTANT: Negative testing is critical — the type checker's whole purpose is to catch version-incorrect code, so we must verify it actually rejects wrong code.",
      "passed": true
    },
    {
      "description": "Create Version API Surface reference JSON files for all library versions",
      "subtasks": [
        "Create `reference/next/v13.json` with valid sync/async APIs, import paths, unavailable APIs, param types — based on BENCHMARK.md Appendix A example",
        "Create `reference/next/v14.json` with Next.js 14 API surface: still sync cookies/headers, direct params type, middleware.ts naming",
        "Create `reference/next/v15.json` with Next.js 15 API surface: async cookies/headers with sync fallback, Promise params type, middleware.ts, after() available",
        "Create `reference/next/v16.json` with Next.js 16 API surface: enforced async, proxy.ts, cacheTag/cacheLife/updateTag, no next lint — based on BENCHMARK.md Appendix A example",
        "Create `reference/react/v17.json` with React 17 API surface: useState, useEffect, useRef, useContext, etc. No useId, useTransition, use(), useActionState. ReactDOM.render. forwardRef required.",
        "Create `reference/react/v18.json` with React 18 API surface: adds useId, useTransition, useDeferredValue, useSyncExternalStore. createRoot replaces render. forwardRef still required.",
        "Create `reference/react/v19.json` with React 19 API surface: adds use(), useActionState, useFormStatus, ref as prop, removes forwardRef requirement, removes ReactDOM.render, removes defaultProps, PropTypes",
        "Create `reference/ai-sdk/v3.json` with AI SDK v3 surface: experimental_ prefixes, await required, toAIStreamResponse, ExperimentalMessage, TokenUsage types",
        "Create `reference/ai-sdk/v4.json` with AI SDK v4 surface: no experimental_ prefix, streamText sync, toDataStreamResponse, CoreMessage, LanguageModelUsage, maxSteps",
        "Create `reference/ai-sdk/v5.json` with AI SDK v5 surface: createUIMessageStream, createUIMessageStreamResponse, writer.merge, data parts with transient flag",
        "Create `reference/trpc/v10.json` with tRPC v10 surface: createTRPCProxyClient, client-level transformer, rawInput in middleware, createProxySSGHelpers, wsLink for subscriptions",
        "Create `reference/trpc/v11.json` with tRPC v11 surface: createTRPCClient, link-level transformer, getRawInput(), createSSGHelpers, httpSubscriptionLink, shorthand routers, async generators",
        "Create `reference/zod/v3.json` with Zod v3 surface: z.string().email(), .url(), .uuid(), .ip(), required_error, invalid_type_error, message param, z.record() single arg, .deepPartial(), .format(), .flatten()",
        "Create `reference/zod/v4.json` with Zod v4 surface: z.email(), z.url(), z.uuid(), z.ipv4(), error param (function), no required_error/invalid_type_error, z.record() requires 2 args, no .deepPartial(), z.treeifyError()",
        "Define a Zod schema for the reference JSON format in `src/types/reference.ts` and validate all reference files load correctly",
        "Write a validation test `src/types/__tests__/reference.test.ts` that loads every reference JSON file and verifies: (1) all files parse against the Zod schema, (2) no two versions of the same library list the same API as both available and unavailable, (3) cross-version consistency checks (e.g., if React 19 lists `use()` as available, React 17 and 18 should list it as unavailable)",
        "Spot-check accuracy: for at least 2 libraries, manually verify 3-5 key API entries against actual library changelogs or documentation (use Nia search to look up the actual docs). Document verification results as comments in the test file",
        "Run `bun test` to verify all reference validation tests pass",
        "Run `bun run typecheck` to ensure no type errors",
        "Run `bun run lint` to ensure code quality"
      ],
      "notes": "These reference files serve as ground truth for both automated AST checks and as context provided to the LLM judge. They list valid APIs, unavailable APIs, and version-specific behaviors. Research each library's actual changelog/migration guide to ensure accuracy. The format should follow BENCHMARK.md Appendix A structure. These files are critical for correctness — errors here propagate to all evaluations. The cross-version consistency check is important: if an API is 'available' in v19 but 'unavailable' in v17, that's correct; if it's 'available' in both v17 AND v19 but the test_spec says it should be absent in v17, there's a conflict.",
      "passed": true
    },
    {
      "description": "Build the opencode agent runner with per-condition MCP configs and temp directory sandboxing",
      "subtasks": [
        "Create `src/runner/mcp_configs/baseline.opencode.json` — opencode config with Claude Sonnet as model, NO MCP servers configured",
        "Create `src/runner/mcp_configs/context7.opencode.json` — opencode config with Claude Sonnet as model, Context7 MCP server configured with `resolve-library-id` and `query-docs` tools (stdio transport: `npx @context7/mcp`)",
        "Create `src/runner/mcp_configs/nia.opencode.json` — opencode config with Claude Sonnet as model, Nia MCP server configured with full toolset (search, nia_read, nia_grep, nia_explore, nia_package_search_hybrid, etc.)",
        "Create `src/runner/agent.ts` with the `runAgent(task: Task, condition: 'baseline' | 'context7' | 'nia', runIndex: number): Promise<AgentResult>` function",
        "Implement temp directory sandboxing: for each execution, create a unique temp dir at `/tmp/nia-bench/{timestamp}-{taskId}-{condition}-{rep}/`",
        "Copy the condition-specific `.opencode.json` config into the temp dir (opencode loads config from CWD)",
        "Implement context injection: if the task has a `context` field (e.g., package.json, existing code files for version-locked tasks), write those files into the temp dir to simulate a real project workspace",
        "Implement opencode CLI invocation using `--cwd` flag: spawn `opencode --cwd <tempdir> -p \"<prompt>\" -f json` using Bun's `Bun.spawn()`, capture stdout as JSON. Do NOT use `cd` — use the `--cwd` flag to set the working directory",
        "Implement dual code extraction strategy: (1) parse the agent's JSON stdout response for markdown code blocks (```typescript...```) using regex, (2) scan the temp dir for any `.ts`, `.tsx`, `.js`, `.jsx` files the agent wrote to disk. Prefer files on disk when available (more complete), fall back to code blocks from stdout",
        "Handle multi-file extraction: some tasks expect multiple files (e.g., page.tsx + actions.ts). Store extracted code as a map of `{filename: code}` rather than a single string",
        "Define `AgentResult` type: `{taskId: string, condition: string, runIndex: number, rawOutput: string, extractedFiles: Record<string, string>, exitCode: number, durationMs: number, workDir: string}`",
        "Handle error cases: opencode not found on PATH (check with `which opencode` at startup), timeout (configurable, default 5 min per task), non-zero exit code, empty response, JSON parse failure",
        "Implement temp dir cleanup: remove the temp dir after code extraction. Add `--keep-workdirs` support by checking a passed-in config flag — if true, skip cleanup and log the path for debugging",
        "Create `src/runner/index.ts` re-exporting the agent runner",
        "Test with a dry-run that verifies: temp dir creation, config injection, context file writing, and cleanup all work correctly (without actually calling opencode)",
        "Run `bun run typecheck` to ensure no type errors",
        "Run `bun run lint` to ensure code quality"
      ],
      "notes": "SANDBOXING: opencode in non-interactive mode auto-approves ALL permissions (file write, bash execution). There is NO built-in sandbox. We mitigate via temp directory isolation — each execution gets its own unique temp dir, and opencode's `--cwd` flag sets it as the working directory. The agent CAN theoretically escape via absolute paths or `../` but Claude Sonnet rarely does this. For maximum security, users can run inside Docker.\n\nCODE EXTRACTION: The agent may (1) output code in its response text as markdown fences, AND/OR (2) actually write files to disk using its write tool. We check BOTH sources. Files on disk are preferred because they're more complete (agent may truncate in text output). Multi-file support is needed for tasks like A-NX-2 that require page.tsx + default.tsx.\n\nCONFIG: opencode searches for `.opencode.json` in CWD first, then home dir. By placing our config in the temp dir, it takes priority. The config uses `mcpServers` field for MCP server definitions with stdio or sse transport types.\n\nSee SPEC.md Architecture > Agent Sandboxing section.\n\nIMPLEMENTATION NOTES: opencode uses `-c` (short for `--cwd`) not `--cwd`. Uses `-q` (quiet) to suppress spinner. JSON output format is `{response: \"...\"}`. The agent runner is fully self-contained per execution — safe for parallel use. All 3 MCP configs use `claude-4-sonnet` model for fair comparison.",
      "passed": true
    },
    {
      "description": "Build the LLM judge module with structured rubric evaluation via OpenRouter",
      "subtasks": [
        "Create `src/judge/prompt-template.ts` with the judge prompt builder function `buildJudgePrompt(task: Task, generatedCode: string, referenceDoc: object): string` that constructs the prompt from BENCHMARK.md Section 5.2 template",
        "Include in the prompt: task prompt, target library version, reference documentation (from reference/ files), reference solution, generated code, rubric criteria, and known hallucination patterns",
        "Create `src/judge/openrouter-client.ts` with a function to call OpenRouter API using the OpenAI-compatible SDK: model `openai/gpt-5-mini`, temperature 0.0, response format expecting JSON",
        "Create `src/judge/rubric-scorer.ts` with `scoreWithRubric(task: Task, generatedCode: string): Promise<JudgeResult>` that calls the LLM judge 3 times, collects per-criterion verdicts, and applies majority vote per criterion",
        "Define `JudgeResult` type: `{criteria: CriterionResult[], judgeScore: number, rawResponses: JudgeResponse[]}` where `CriterionResult` has `{name: string, verdict: 'PASS' | 'FAIL', weight: number, evidence: string, reasoning: string}`",
        "Implement majority voting: for each criterion across 3 runs, take the majority verdict (PASS if >=2 PASS, FAIL otherwise). Use evidence/reasoning from the majority side.",
        "Calculate `judgeScore` as weighted sum of passed criteria: `sum(passed_criterion.weight) / sum(all_criterion.weight)`",
        "Handle JSON parsing errors from the LLM: if response isn't valid JSON, retry once. If still invalid, mark all criteria as FAIL for that run.",
        "Create `src/judge/index.ts` re-exporting the module",
        "Write test `src/judge/__tests__/prompt-template.test.ts`: verify the built prompt includes all required sections: task prompt, target version, reference docs, reference solution, generated code, rubric criteria, known hallucinations, and the 'ONLY use reference documentation' instruction",
        "Write test `src/judge/__tests__/rubric-scorer.test.ts` for majority voting logic (no API calls — test the voting function in isolation):",
        "Test case 1 (unanimous PASS): 3 runs all return PASS for criterion X — verify final verdict is PASS",
        "Test case 2 (majority PASS): 2 PASS + 1 FAIL — verify final verdict is PASS, evidence comes from a PASS run",
        "Test case 3 (majority FAIL): 1 PASS + 2 FAIL — verify final verdict is FAIL",
        "Test case 4 (score calculation): 3 criteria with weights 30%, 30%, 40%. Two pass, one fails (the 40% one). Verify judgeScore = 0.6",
        "Test case 5 (JSON parse error handling): simulate an invalid JSON response — verify it retries and falls back to all-FAIL gracefully",
        "Run `bun test` to verify all judge tests pass",
        "Run `bun run typecheck` to ensure no type errors",
        "Run `bun run lint` to ensure code quality"
      ],
      "notes": "OpenRouter uses the OpenAI-compatible API. Set base URL to `https://openrouter.ai/api/v1` and use `OPENROUTER_API_KEY`. The model ID on OpenRouter is `openai/gpt-5-mini`. Temperature must be 0.0 for reproducibility. The prompt template is in BENCHMARK.md Section 5.2 — follow it exactly. The judge must be instructed to ONLY use the reference documentation provided, not its own knowledge. Majority vote across 3 runs reduces scoring variance. See SPEC.md constraints.",
      "passed": false
    },
    {
      "description": "Build the hallucination classifier module",
      "subtasks": [
        "Create `src/judge/hallucination-classifier.ts` with a function `classifyHallucinations(task: Task, generatedCode: string, astResults: AstCheckResult[], judgeResult: JudgeResult): HallucinationType[]`",
        "Define `HallucinationType` enum: `invented_method`, `wrong_parameter`, `outdated_api`, `future_api`, `wrong_import_path`, `version_mismatch`",
        "Implement classification based on failed AST checks: map specific check failures to hallucination types (e.g., `import_absent` failure for a newer API -> `future_api`, `call_absent` failure for a removed API -> `outdated_api`)",
        "Implement classification from judge results: extract hallucination signals from FAIL verdicts on `no_hallucination` criteria",
        "Cross-reference with the task's `common_hallucinations` list to label known failure patterns",
        "Create `HallucinationResult` type: `{types: HallucinationType[], details: {type: HallucinationType, evidence: string, description: string}[]}`",
        "Export the classifier from `src/judge/index.ts`",
        "Write test `src/judge/__tests__/hallucination-classifier.test.ts` with the following cases:",
        "Test case 1 (future_api): mock a Next.js 13 task where AST check `await_absent` for `cookies()` failed (agent used `await cookies()`) — verify classifier outputs `future_api` type",
        "Test case 2 (outdated_api): mock a React 19 task where AST check `import_absent` for `forwardRef` failed — verify classifier outputs `outdated_api` type",
        "Test case 3 (wrong_import_path): mock a React 19 task where `useActionState` was imported from `react-dom` instead of `react` — verify classifier outputs `wrong_import_path` type",
        "Test case 4 (multiple hallucinations): mock a task with multiple AST failures — verify classifier correctly returns multiple hallucination types",
        "Test case 5 (no hallucinations): mock a task where all AST checks pass and judge gives all PASS — verify classifier returns empty types array",
        "Run `bun test` to verify all hallucination classifier tests pass",
        "Run `bun run typecheck` to ensure no type errors",
        "Run `bun run lint` to ensure code quality"
      ],
      "notes": "The hallucination taxonomy is defined in BENCHMARK.md Section 5.4. The classifier combines signals from both automated tests (AST failures indicating wrong APIs) and judge verdicts (semantic evaluation of correctness). Each task's `common_hallucinations` field lists expected failure modes — use these to provide specific labels. A single code sample can have multiple hallucination types simultaneously. TESTING IS CRITICAL: this module has no external dependencies (no API calls) so there's no excuse not to have comprehensive unit tests. The mock data for tests should use realistic AST check results and judge results that mirror actual failure patterns described in BENCHMARK.md.",
      "passed": false
    },
    {
      "description": "Build the combined evaluator that orchestrates AST checks, type checking, and LLM judge scoring",
      "subtasks": [
        "Create `src/runner/evaluator.ts` with function `evaluateCode(task: Task, extractedFiles: Record<string, string>): Promise<EvaluationResult>` that runs all evaluation layers on the extracted code files",
        "Handle multi-file input: some tasks produce multiple files (e.g., page.tsx + actions.ts + default.tsx). Determine which file(s) to run AST checks on based on the task's test_spec (each AST check should specify which file it applies to, or default to the primary file)",
        "Concatenate all extracted files for LLM judge evaluation (the judge needs full context)",
        "Layer 1: Run AST checks via `runAstChecks()` on the appropriate files — compute `testScore = passedChecks / totalChecks`",
        "Layer 1b: Run type check via `runTypeCheck()` if the task has `test_spec.type_check: true` — include as an additional pass/fail assertion in the test score",
        "Layer 2: Run LLM judge via `scoreWithRubric()` — get `judgeScore` (0.0-1.0)",
        "Compute combined score: `finalScore = 0.6 * testScore + 0.4 * judgeScore`",
        "Special case for audit tasks (category: version_locked_audit): these have no AST checks (empty array). When no AST checks exist, `finalScore = judgeScore` (100% judge weight)",
        "Run hallucination classifier with all signals",
        "Define `EvaluationResult` type: `{taskId: string, condition: string, runIndex: number, testScore: number, judgeScore: number, finalScore: number, astResults: AstCheckResult[], typeCheckResult: TypeCheckResult | null, judgeResult: JudgeResult, hallucinations: HallucinationResult, extractedFiles: Record<string, string>}`",
        "Add option to skip LLM judge (for faster iteration during development) — in that case, `finalScore = testScore` (or 0 for audit tasks with no AST checks)",
        "Write integration test `src/runner/__tests__/evaluator.test.ts` with the following cases:",
        "Test case 1 (skip-judge mode): use a pilot task + its reference solution code, run evaluator with `skipJudge: true`. Verify: testScore is 1.0 (all AST checks pass), judgeScore is 0, finalScore equals testScore, no hallucinations detected",
        "Test case 2 (skip-judge, bad code): use a pilot task + known-bad hallucinated code, run evaluator with `skipJudge: true`. Verify: testScore < 1.0, some AST checks fail, hallucination types are populated",
        "Test case 3 (audit task, skip-judge): use an audit pilot task (no AST checks), run evaluator with `skipJudge: true`. Verify: finalScore is 0 (since no AST checks and judge is skipped), not crashing",
        "Test case 4 (score formula): mock AST results (4/5 pass = 0.8) and mock judge result (0.6). Verify: finalScore = 0.6 * 0.8 + 0.4 * 0.6 = 0.72",
        "Run `bun test` to verify all evaluator tests pass",
        "Export the evaluator from `src/runner/index.ts`",
        "Run `bun run typecheck` to ensure no type errors",
        "Run `bun run lint` to ensure code quality"
      ],
      "notes": "The combined score formula is `0.6 * test_score + 0.4 * judge_score` as specified in BENCHMARK.md Section 5.3, EXCEPT for audit tasks which rely entirely on the judge (no AST checks possible for free-text analysis responses). The evaluator accepts `extractedFiles: Record<string, string>` (filename -> code) from the agent runner, supporting both single and multi-file outputs. The evaluator should be independent from the agent runner — it takes extracted files as input so evaluation can be re-run on existing outputs. The skip-judge flag is useful for development/testing. Integration tests use skipJudge mode to avoid real API calls — the judge module itself was tested in task 8. See SPEC.md Architecture Phase 3.",
      "passed": false
    },
    {
      "description": "Build the CLI orchestrator with configurable parallelism, work queue, and result storage",
      "subtasks": [
        "Expand `src/index.ts` into a full CLI using Bun's `process.argv` parsing (or a lightweight arg parser like `minimist`): support flags `--category`, `--library`, `--task`, `--condition`, `--reps` (default 3), `--parallel` (default 1), `--skip-judge`, `--keep-workdirs`, `--output-dir` (default `results/`), `--timeout` (default 300000ms = 5 min per agent execution)",
        "Implement work queue generation: enumerate all (task, condition, rep) tuples into a flat array of work items",
        "Implement execution order randomization: shuffle the work queue to prevent ordering bias (per SPEC.md Constraints). Use a seeded random for reproducibility — accept `--seed` flag",
        "Implement parallel worker pool: create an async semaphore/pool that runs up to `--parallel N` work items concurrently. Each worker: (1) pulls next item from queue, (2) calls `runAgent()` which creates its own temp dir, (3) calls `evaluateCode()`, (4) calls `storeResult()`. Workers are fully independent — no shared mutable state",
        "Use `Promise.allSettled` with a concurrency limiter (e.g., a simple async semaphore using a counter + promise queue, or use `p-limit` if preferred) to manage the worker pool",
        "Create `src/runner/result-store.ts` that writes each `EvaluationResult` as a JSON file to `results/{timestamp}/{taskId}/{condition}/run-{index}.json`",
        "Write a run metadata file `results/{timestamp}/run-meta.json` with: start time, end time, total tasks, conditions, reps, parallel level, seed, CLI args, opencode version",
        "Implement thread-safe progress logging: use an atomic counter for completed items. Print `[12/333] Task: nextjs-16-proxy-ts | Condition: nia | Rep: 2/3 | Elapsed: 5m23s | ETA: 2h15m` — include elapsed time and ETA based on average execution time so far",
        "Handle interruption gracefully (SIGINT/SIGTERM): on signal, stop spawning new workers, wait for in-flight workers to complete, save all results collected so far, write partial run-meta.json with `status: 'interrupted'`",
        "Add `--eval-only` flag that skips agent execution and re-runs evaluation on existing agent outputs in a specified results directory",
        "Add `--report-only` flag that skips execution and evaluation, just generates the report from existing results",
        "Add `--dry-run` flag that prints the execution plan (shuffled work queue with task/condition/rep) without executing anything — useful for verifying the plan",
        "Verify --dry-run: run `bun run bench --dry-run --reps 1` and confirm it prints exactly 5×3×1 = 15 work items (for the 5 pilot tasks), each showing task ID, condition, and rep index, then exits without calling opencode",
        "Verify --dry-run with filters: run `bun run bench --dry-run --task nextjs-16-proxy-ts --condition baseline --reps 2` and confirm it prints exactly 1×1×2 = 2 work items",
        "Verify --seed reproducibility: run `bun run bench --dry-run --reps 1 --seed 42` twice and confirm the execution order is identical both times",
        "Verify work queue generation: write a test `src/runner/__tests__/orchestrator.test.ts` that creates a work queue from 3 tasks × 2 conditions × 2 reps and verifies: (1) total items = 12, (2) all task/condition/rep combos are present, (3) shuffling with same seed produces same order, (4) shuffling with different seed produces different order",
        "Run `bun test` to verify orchestrator tests pass",
        "Run `bun run typecheck` to ensure no type errors",
        "Run `bun run lint` to ensure code quality"
      ],
      "notes": "PARALLELISM: The main bottleneck is the LLM provider's API rate limit (Anthropic for Claude Sonnet). With --parallel 5, we'd have 5 simultaneous Claude Sonnet requests. Users should set --parallel based on their API tier. Default is 1 (sequential) for safety.\n\nCONCURRENCY SAFETY: Each worker operates in its own temp dir (created by runAgent), so there are zero file system conflicts. opencode processes in different directories have no shared state (confirmed by code research — separate SQLite DBs, config, and sessions per CWD). The only shared resource is the results directory — use atomic file writes (write to temp file then rename) to avoid corruption.\n\nWORK QUEUE: The queue is generated upfront: for N tasks × 3 conditions × R reps = N*3*R items. Shuffling ensures no systematic ordering bias. The --seed flag allows reproducing the exact same execution order.\n\nETA CALCULATION: Track a rolling average of the last 10 execution durations to estimate remaining time. This is more accurate than total average since early tasks may be faster/slower.\n\nVERIFICATION: The --dry-run tests validate the orchestrator logic without any API calls. This is the cheapest and fastest way to verify the core orchestration works. The actual E2E execution is tested in the pilot run (task 12).",
      "passed": false
    },
    {
      "description": "Pilot run: execute end-to-end pipeline on 5 pilot tasks and validate sandboxing, execution, and evaluation",
      "subtasks": [
        "Ensure opencode CLI is installed and available on PATH (`which opencode`)",
        "Ensure OpenRouter API key is set in `.env` for the LLM judge",
        "Run `bun run bench --dry-run --reps 1` to verify the execution plan is generated correctly (shuffled work queue, 15 items for 5 tasks × 3 conditions)",
        "Run `bun run bench --task nextjs-16-proxy-ts --condition baseline --reps 1 --skip-judge --keep-workdirs` to test agent execution + temp dir sandboxing + AST evaluation",
        "Inspect the kept temp dir: verify `.opencode.json` was injected, check if agent wrote any files to disk, verify code extraction found the generated code from both stdout and disk",
        "Verify the agent output is captured, code is extracted, AST checks run, and result JSON is written correctly",
        "Run `bun run bench --task nextjs-16-proxy-ts --condition baseline --reps 1` (with judge, without --keep-workdirs) to test the full pipeline including LLM judge and temp dir cleanup",
        "Verify the judge calls succeed, rubric scoring works, majority voting produces results, hallucination classification runs, and temp dir is cleaned up",
        "Test parallel execution: `bun run bench --reps 1 --parallel 3 --skip-judge` to run 5 pilot tasks × 3 conditions with 3 workers simultaneously. Verify no file conflicts or race conditions in result storage",
        "Run all 5 pilot tasks with all 3 conditions, 1 rep, and full judge: `bun run bench --reps 1`",
        "Review the 15 result files (5 tasks × 3 conditions) — verify data structure, scores, and that the extractedFiles field contains the correct files",
        "Fix any bugs discovered during the pilot run",
        "Document any issues or adjustments needed in the task schema, AST checks, code extraction, or judge prompts",
        "Run `bun run typecheck` to ensure no type errors",
        "Run `bun run lint` to ensure code quality"
      ],
      "notes": "This is the critical validation step before authoring all remaining tasks. Run with --reps 1 to minimize cost. The goal is to validate: (1) temp dir sandboxing works — config injection, context files, agent file writes, cleanup, (2) opencode invocation works for all 3 conditions with correct MCP configs, (3) dual code extraction (stdout + disk files) handles real agent outputs, (4) parallel execution doesn't cause conflicts, (5) AST checker produces meaningful results, (6) LLM judge returns parseable JSON via OpenRouter, (7) results are saved correctly with the new extractedFiles format. Expect to find and fix issues — this is the whole point of piloting early. Budget ~$5-10 in API costs for this step.",
      "passed": false
    },
    {
      "description": "Author all remaining Bleeding-Edge task JSON files (9 remaining after 2 pilot tasks)",
      "subtasks": [
        "Create `tasks/bleeding_edge/nextjs-16-enforced-async.json` — Task A-NX-2: enforced async APIs + parallel route defaults. Copy from BENCHMARK.md Section 4.1",
        "Create `tasks/bleeding_edge/nextjs-16-cache-components.json` — Task A-NX-3: 'use cache' directive + cacheTag/cacheLife/updateTag",
        "Create `tasks/bleeding_edge/react-19-form-actions.json` — Task A-RX-2: useActionState + useFormStatus",
        "Create `tasks/bleeding_edge/react-19-ref-as-prop.json` — Task A-RX-3: ref as prop, no forwardRef",
        "Create `tasks/bleeding_edge/ai-sdk-5-ui-message-stream.json` — Task A-AI-1: createUIMessageStream/createUIMessageStreamResponse",
        "Create `tasks/bleeding_edge/ai-sdk-5-data-parts.json` — Task A-AI-2: data parts with transient state",
        "Create `tasks/bleeding_edge/ai-sdk-4-sync-stream-text.json` — Task A-AI-3: streamText without await in v4",
        "Create `tasks/bleeding_edge/trpc-11-transformer-link.json` — Task A-TR-1: transformer in link config",
        "Create `tasks/bleeding_edge/trpc-11-sse-subscriptions.json` — Task A-TR-2: SSE subscriptions with httpSubscriptionLink",
        "Create `tasks/bleeding_edge/trpc-11-shorthand-streaming.json` — Task A-TR-3: shorthand router + streaming query",
        "Create `tasks/bleeding_edge/zod-4-top-level-validators.json` — Task A-ZD-1: z.email(), z.url(), z.uuid(), z.ipv4()",
        "Create `tasks/bleeding_edge/zod-4-error-api.json` — Task A-ZD-2: error customization API overhaul",
        "Validate all 14 bleeding-edge tasks load with the task loader",
        "Run AST checks on EVERY task's reference_solution code and verify all checks PASS. This catches broken test_spec definitions before they reach the benchmark run. Write a verification script or test that iterates all 14 tasks, runs `runAstChecks(task.reference_solution, task.test_spec.ast_checks)`, and asserts 100% pass rate",
        "Run `bun run typecheck` to ensure no type errors",
        "Run `bun run lint` to ensure code quality"
      ],
      "notes": "All task content (prompts, reference solutions, test specs, rubrics, common hallucinations) is fully defined in BENCHMARK.md Section 4.1. Copy verbatim but ensure the JSON structure matches our Zod schema. Double-check that test_spec AST checks cover all assertions listed in BENCHMARK.md for each task. The 2 pilot tasks (nextjs-16-proxy-ts, react-19-use-hook) are already done. CRITICAL VERIFICATION: Every task's reference solution MUST pass all its own AST checks. If it doesn't, either the reference solution or the AST check is wrong — fix before proceeding.",
      "passed": false
    },
    {
      "description": "Author all remaining Version-Locked Write task JSON files (12 remaining after 2 pilot tasks)",
      "subtasks": [
        "Create `tasks/version_locked_write/nextjs-14-direct-params.json` — Task B1-NX-2: direct params/searchParams access in v14",
        "Create `tasks/version_locked_write/nextjs-15-middleware-ts.json` — Task B1-NX-3: middleware.ts (not proxy.ts) in v15",
        "Create `tasks/version_locked_write/react-17-data-fetching.json` — Task B1-RX-1: useEffect data fetching in v17",
        "Create `tasks/version_locked_write/react-18-forward-ref.json` — Task B1-RX-3: forwardRef in React 18",
        "Create `tasks/version_locked_write/ai-sdk-3-async-stream.json` — Task B1-AI-1: experimental_streamText with await in v3",
        "Create `tasks/version_locked_write/ai-sdk-3-type-names.json` — Task B1-AI-2: ExperimentalMessage, TokenUsage v3 type names",
        "Create `tasks/version_locked_write/trpc-10-client-transformer.json` — Task B1-TR-1: client-level transformer in v10",
        "Create `tasks/version_locked_write/trpc-10-middleware-raw-input.json` — Task B1-TR-2: rawInput (not getRawInput) in v10 middleware",
        "Create `tasks/version_locked_write/trpc-10-ssg-helpers.json` — Task B1-TR-3: createProxySSGHelpers in v10",
        "Create `tasks/version_locked_write/zod-3-chained-validators.json` — Task B1-ZD-1: z.string().email(), z.string().url(), etc.",
        "Create `tasks/version_locked_write/zod-3-error-message.json` — Task B1-ZD-2: required_error, invalid_type_error, message param",
        "Create `tasks/version_locked_write/zod-3-record-single-arg.json` — Task B1-ZD-3: z.record(z.string()) single argument",
        "Validate all 14 version-locked-write tasks load with the task loader",
        "Run AST checks on EVERY task's reference_solution code and verify all checks PASS. Reuse the same verification approach from the bleeding-edge task authoring — iterate all 14 tasks, run AST checks, assert 100% pass rate",
        "Run `bun run typecheck` to ensure no type errors",
        "Run `bun run lint` to ensure code quality"
      ],
      "notes": "All task content is in BENCHMARK.md Section 4.2. The 2 pilot tasks (nextjs-13-sync-request-apis, react-17-render-entry) are already done. Version-locked write tasks must include the `context` field with relevant package.json snippets showing the pinned version, so the agent knows which version to target. CRITICAL VERIFICATION: Every task's reference solution MUST pass all its own AST checks.",
      "passed": false
    },
    {
      "description": "Author all remaining Version-Locked Audit task JSON files (8 remaining after 1 pilot task)",
      "subtasks": [
        "Create `tasks/version_locked_audit/nextjs-13-audit-v16-code.json` — Task B2-NX-1: audit v16 code for v13 compatibility. Include the code snippet in the prompt",
        "Create `tasks/version_locked_audit/nextjs-16-audit-v15-code.json` — Task B2-NX-2: audit v15 code upgrading to v16",
        "Create `tasks/version_locked_audit/nextjs-16-audit-parallel-routes.json` — Task B2-NX-3: audit missing parallel route defaults",
        "Create `tasks/version_locked_audit/react-19-audit-removed-apis.json` — Task B2-RX-2: detect removed APIs in React 19",
        "Create `tasks/version_locked_audit/react-18-audit-missed-features.json` — Task B2-RX-3: audit React 17 code for v18 migration",
        "Create `tasks/version_locked_audit/zod-4-audit-v3-code.json` — Task B2-ZD-1: audit Zod v3 code for v4 migration",
        "Create `tasks/version_locked_audit/trpc-11-audit-v10-code.json` — Task B2-TR-1: audit tRPC v10 code for v11 migration",
        "Create `tasks/version_locked_audit/ai-sdk-4-audit-v3-code.json` — Task B2-AI-1: audit AI SDK v3 code for v4 migration",
        "Validate all 9 version-locked-audit tasks load with the task loader",
        "Verify all audit tasks have `test_spec.ast_checks` set to an empty array `[]` — audit tasks rely entirely on the LLM judge",
        "Verify all audit tasks have non-empty rubric criteria — since audit tasks are 100% judge-evaluated, the rubric is the only evaluation mechanism and must be comprehensive",
        "Verify each audit task's rubric criteria weights sum to approximately 100% (allowing for rounding)",
        "Run `bun run typecheck` to ensure no type errors",
        "Run `bun run lint` to ensure code quality"
      ],
      "notes": "All task content is in BENCHMARK.md Sections 4.3. The 1 pilot task (react-17-audit-v19-code) is already done. Audit tasks have a different structure: the prompt includes code to audit, and the reference_solution lists expected issues. The rubric criteria map to specific issues that should be identified. Audit tasks do NOT have code-generation AST checks — instead, the LLM judge evaluates whether the agent identified the correct issues and provided correct alternatives. For audit tasks, set `test_spec.ast_checks` to an empty array and rely solely on the judge (`finalScore = judgeScore`). VERIFICATION: Since audit tasks have no AST checks, we verify structural correctness instead — empty ast_checks, non-empty rubric, correct weight distribution.",
      "passed": false
    },
    {
      "description": "Build the results reporter generating comparison tables and per-task breakdowns",
      "subtasks": [
        "Create `src/runner/reporter.ts` with a function `generateReport(resultsDir: string): Report` that reads all result JSON files and aggregates scores",
        "Implement primary metrics computation: Task Pass Rate (final_score >= 0.8), Hallucination Rate (>= 1 hallucination), Version Compliance Rate (all AST checks pass), Mean Combined Score",
        "Implement per-condition aggregation: compute all metrics separately for Baseline, Context7, Nia",
        "Implement per-category breakdown: separate metrics for bleeding_edge, version_locked_write, version_locked_audit",
        "Implement per-library breakdown: separate metrics for next, react, ai, trpc, zod",
        "Implement hallucination type distribution: count and percentage of each HallucinationType across all conditions",
        "Implement per-task detail view: for each task, show all 3 conditions' scores side-by-side with pass/fail on individual criteria",
        "Generate formatted console output matching BENCHMARK.md Section 7.3 output format (the ASCII table)",
        "Write the full report as JSON to `results/{timestamp}/report.json` for programmatic access",
        "Also write a human-readable `results/{timestamp}/report.txt` with the formatted table",
        "Handle edge cases: partial results (some tasks missing), single condition only, single rep",
        "Write test `src/runner/__tests__/reporter.test.ts` using mock result data:",
        "Test case 1 (basic report): create a temp dir with 6 mock result files (2 tasks × 3 conditions × 1 rep) with known scores. Run `generateReport()` and verify: Task Pass Rate, Hallucination Rate, Mean Combined Score are calculated correctly for each condition",
        "Test case 2 (per-category breakdown): mock results spanning bleeding_edge and version_locked_write categories. Verify the breakdown separates them correctly",
        "Test case 3 (partial results): create a results dir with only 1 condition present. Verify the reporter handles missing conditions gracefully (shows N/A or similar) without crashing",
        "Test case 4 (report output): verify both `report.json` and `report.txt` are written, the JSON is valid and parseable, and the text matches the expected ASCII table format",
        "Run `bun test` to verify all reporter tests pass",
        "Run `bun run typecheck` to ensure no type errors",
        "Run `bun run lint` to ensure code quality"
      ],
      "notes": "The report format is defined in BENCHMARK.md Section 7.3 — follow it exactly for the console output. The JSON version should be a structured object enabling further analysis. When computing averages across reps (3 per task×condition), use the mean of final scores. The reporter must be callable standalone via `bun run bench --report-only --output-dir results/{timestamp}`. TESTING: The reporter is a pure data-transformation module (reads JSON files, computes metrics, writes output) with no external API calls — it should have thorough unit tests with mock data. See SPEC.md Architecture Phase 4.",
      "passed": false
    }
  ]
}
